# Optimization Course IMA-314

Welcome to the repository for the Optimization Course IMA-314! This course covers fundamental concepts and techniques in optimization, focusing primarily on first-order and second-order optimization methods.

## Course Overview

In this course, we will explore various optimization algorithms, including:

- **Gradient Descent (GD)**: A first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
- **Newton's Gradient Descent (NGD)**: An extension of GD that uses second-order derivatives to improve convergence rates.
- **Adagrad**: An adaptive learning rate method that adjusts the learning rate based on the parameters.
- **RMSProp**: A modification of Adagrad that helps mitigate the rapid decay of the learning rate.
- **Adam**: Combines the advantages of both Adagrad and RMSProp, using moments to enhance the convergence.

![Optimization GIF](http://gifgifs.com/animations/transportation/rockets-and-shuttles/Large_rocket.gif)  <!-- Replace with a relevant GIF URL -->

## Getting Started

To get started with the course materials and exercises, clone this repository:

```bash
git clone https://github.com/Harshtherocking/Optimization-IMA-314/tree/main
```

## Contributing

Feel free to contribute by submitting issues or pull requests. We welcome any improvements, suggestions, or additional resources!

## License

This project is licensed under the MIT License. See the LICENSE file for details.

---
